# -*- coding: utf-8 -*-
"""
DQN v2: Sim-to-Real í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ íŒŒì´í”„ë¼ì¸
==============================================
Phase A: XGB ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ 200K íƒ€ì„ìŠ¤í… ê³ ì† í•™ìŠµ
Phase B: ì‹¤ì œ DB í™˜ê²½ì—ì„œ 10K íƒ€ì„ìŠ¤í… Fine-tuning

ì „ëµ:
- ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì•ˆì „í•˜ê³  ë¹ ë¥´ê²Œ ê¸°ë³¸ ì •ì±… í•™ìŠµ
- ì‹¤ì œ í™˜ê²½ì—ì„œ ì‹œë®¬ë ˆì´ì…˜ ì˜¤ì°¨ ë³´ì • ë° ì‹¤ì „ ìµœì í™”
"""

import os
import sys
from datetime import datetime
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.join(os.getcwd(), 'Apollo.ML'))

from RLQO.env.v2_sim_env import QueryPlanSimEnv
from RLQO.env.v2_db_env import QueryPlanDBEnvV2
from RLQO.constants import SAMPLE_QUERIES

# ============================================================================
# Phase A: ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ì„¤ì •
# ============================================================================
SIM_TIMESTEPS = 200_000  # ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµëŸ‰ (ë¹ ë¦„: ì˜ˆìƒ 1-2ì‹œê°„)
SIM_LEARNING_RATE = 1e-4
SIM_BUFFER_SIZE = 50_000
SIM_BATCH_SIZE = 128
SIM_EXPLORATION_FRACTION = 0.5  # 50%ê¹Œì§€ë§Œ íƒí—˜
SIM_EXPLORATION_FINAL_EPS = 0.05

# ============================================================================
# Phase B: ì‹¤ì œ DB Fine-tuning ì„¤ì •
# ============================================================================
REAL_TIMESTEPS = 10_000  # ì‹¤ì œ DB í•™ìŠµëŸ‰ (ëŠë¦¼: ì˜ˆìƒ 12-14ì‹œê°„)
REAL_LEARNING_RATE = 5e-5  # ë‚®ì€ í•™ìŠµë¥ ë¡œ ë¯¸ì„¸ ì¡°ì •
REAL_BUFFER_SIZE = 20_000
REAL_BATCH_SIZE = 64
REAL_EXPLORATION_FINAL_EPS = 0.02  # ë” ì ì€ íƒí—˜

# ============================================================================
# ê³µí†µ ì„¤ì •
# ============================================================================
GAMMA = 0.99
TARGET_UPDATE_INTERVAL = 1000

# ê²½ë¡œ ì„¤ì •
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
BASE_DIR = "Apollo.ML/artifacts/RLQO/"

SIM_LOG_DIR = f"{BASE_DIR}logs/dqn_v2_sim/"
SIM_MODEL_PATH = f"{BASE_DIR}models/dqn_v2_sim.zip"
SIM_CHECKPOINT_DIR = f"{BASE_DIR}models/checkpoints/dqn_v2_sim/"

REAL_LOG_DIR = f"{BASE_DIR}logs/dqn_v2_real/"
REAL_MODEL_PATH = f"{BASE_DIR}models/dqn_v2_final.zip"
REAL_CHECKPOINT_DIR = f"{BASE_DIR}models/checkpoints/dqn_v2_real/"

TB_LOG_DIR = f"{BASE_DIR}tb/dqn_v2/"

# ë””ë ‰í† ë¦¬ ìƒì„±
for directory in [SIM_LOG_DIR, SIM_CHECKPOINT_DIR, REAL_LOG_DIR, REAL_CHECKPOINT_DIR]:
    os.makedirs(directory, exist_ok=True)


def train_phase_a_simulation():
    """
    Phase A: ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ê¸°ë³¸ ì •ì±… í•™ìŠµ
    - ë¹ ë¥¸ í•™ìŠµ ì†ë„ (ì‹¤ì œ DBì˜ 100ë°° ì´ìƒ)
    - ì•ˆì „í•œ íƒí—˜ (ì˜ëª»ëœ ì¿¼ë¦¬ë„ ì‹œìŠ¤í…œ ì˜í–¥ ì—†ìŒ)
    - 200K íƒ€ì„ìŠ¤í…: ì•½ 18,000 ì—í”¼ì†Œë“œ (11ê°œ ì¿¼ë¦¬ ê¸°ì¤€)
    """
    print("=" * 80)
    print(" Phase A: ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    print(f"íƒ€ì„ìŠ¤í…: {SIM_TIMESTEPS:,}")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: 1-2ì‹œê°„")
    print(f"ì¿¼ë¦¬ ê°œìˆ˜: {len(SAMPLE_QUERIES)}")
    print("-" * 80)
    
    # 1. ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ìƒì„±
    print("\n[1/4] ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ìƒì„± ì¤‘...")
    try:
        env = QueryPlanSimEnv(
            query_list=SAMPLE_QUERIES,
            xgb_model_path='Apollo.ML/artifacts/model.joblib',
            max_steps=10,
            verbose=False  # í•™ìŠµ ì¤‘ì—ëŠ” ì¶œë ¥ ìµœì†Œí™”
        )
        env = Monitor(env, SIM_LOG_DIR)
        print("[OK] í™˜ê²½ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"[ERROR] í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì •
    print("\n[2/4] ì½œë°± ì„¤ì • ì¤‘...")
    checkpoint_callback = CheckpointCallback(
        save_freq=20_000,  # 20Kë§ˆë‹¤ ì €ì¥
        save_path=SIM_CHECKPOINT_DIR,
        name_prefix="dqn_v2_sim"
    )
    print("[OK] ì½œë°± ì„¤ì • ì™„ë£Œ")
    
    # 3. DQN ëª¨ë¸ ìƒì„±
    print("\n[3/4] DQN ëª¨ë¸ ìƒì„± ì¤‘...")
    model = DQN(
        "MlpPolicy",
        env,
        learning_rate=SIM_LEARNING_RATE,
        buffer_size=SIM_BUFFER_SIZE,
        learning_starts=1000,  # ì¶©ë¶„í•œ ê²½í—˜ ìˆ˜ì§‘ í›„ í•™ìŠµ ì‹œì‘
        batch_size=SIM_BATCH_SIZE,
        gamma=GAMMA,
        exploration_fraction=SIM_EXPLORATION_FRACTION,
        exploration_final_eps=SIM_EXPLORATION_FINAL_EPS,
        train_freq=(1, "step"),
        gradient_steps=1,
        target_update_interval=TARGET_UPDATE_INTERVAL,
        verbose=1,
        tensorboard_log=TB_LOG_DIR
    )
    print("[OK] ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"  Policy: {model.policy}")
    print(f"  Learning Rate: {SIM_LEARNING_RATE}")
    print(f"  Buffer Size: {SIM_BUFFER_SIZE:,}")
    
    # 4. ëª¨ë¸ í•™ìŠµ
    print("\n[4/4] í•™ìŠµ ì‹œì‘...")
    print("-" * 80)
    try:
        model.learn(
            total_timesteps=SIM_TIMESTEPS,
            callback=checkpoint_callback,
            progress_bar=True,
            log_interval=100
        )
        print("\n[OK] í•™ìŠµ ì™„ë£Œ")
    except Exception as e:
        print(f"\n[ERROR] í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    
    # 5. ëª¨ë¸ ì €ì¥
    print(f"\n[5/5] ëª¨ë¸ ì €ì¥ ì¤‘: {SIM_MODEL_PATH}")
    model.save(SIM_MODEL_PATH)
    print("[OK] ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    env.close()
    
    print("\n" + "=" * 80)
    print(" Phase A: ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    
    return model


def train_phase_b_finetuning():
    """
    Phase B: ì‹¤ì œ DB í™˜ê²½ì—ì„œ Fine-tuning
    - Phase Aì—ì„œ í•™ìŠµí•œ ëª¨ë¸ ë¡œë“œ
    - ì‹¤ì œ DBì—ì„œ 10K íƒ€ì„ìŠ¤í… ì¶”ê°€ í•™ìŠµ
    - ì‹œë®¬ë ˆì´ì…˜ ì˜¤ì°¨ ë³´ì • ë° ì‹¤ì „ ìµœì í™”
    """
    print("\n\n")
    print("=" * 80)
    print(" Phase B: ì‹¤ì œ DB Fine-tuning ì‹œì‘")
    print("=" * 80)
    print(f"íƒ€ì„ìŠ¤í…: {REAL_TIMESTEPS:,}")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: 2-3ì‹œê°„")
    print("-" * 80)
    
    # 1. Phase A ëª¨ë¸ ë¡œë“œ
    print("\n[1/5] Phase A ëª¨ë¸ ë¡œë“œ ì¤‘...")
    if not os.path.exists(SIM_MODEL_PATH):
        print(f"[ERROR] Phase A ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SIM_MODEL_PATH}")
        print("  ë¨¼ì € Phase Aë¥¼ ì™„ë£Œí•˜ì„¸ìš”.")
        return None
    
    # 2. ì‹¤ì œ DB í™˜ê²½ ìƒì„± (v2: í™•ì¥ëœ ì•¡ì…˜ ê³µê°„, ì•ˆì „ì„± ì ìˆ˜)
    print("\n[2/5] ì‹¤ì œ DB í™˜ê²½ ìƒì„± ì¤‘...")
    try:
        env = QueryPlanDBEnvV2(
            query_list=SAMPLE_QUERIES,
            max_steps=10,
            curriculum_mode=True,  # Curriculum Learning í™œì„±í™” (ì‰¬ìš´ ì¿¼ë¦¬ë¶€í„° í•™ìŠµ)
            verbose=True
        )
        env = Monitor(env, REAL_LOG_DIR)
        print("[OK] í™˜ê²½ ìƒì„± ì™„ë£Œ (v2.1: 15ê°œ ì•¡ì…˜)")
    except Exception as e:
        print(f"[ERROR] í™˜ê²½ ìƒì„± ì‹¤íŒ¨: {e}")
        print("  DB ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None
    
    # 3. Phase A ëª¨ë¸ ë¡œë“œ ë° í™˜ê²½ ì—°ê²°
    print("\n[3/5] ëª¨ë¸ ë¡œë“œ ë° ì¬ì„¤ì • ì¤‘...")
    try:
        model = DQN.load(SIM_MODEL_PATH, env=env)
        print("[OK] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Fine-tuningì„ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
        model.learning_rate = REAL_LEARNING_RATE
        model.exploration_final_eps = REAL_EXPLORATION_FINAL_EPS
        # Replay BufferëŠ” ìœ ì§€ (ì‹œë®¬ë ˆì´ì…˜ ê²½í—˜ í™œìš©)
        
        print(f"  Learning Rate: {REAL_LEARNING_RATE} (ë‚®ì¶¤)")
        print(f"  Exploration Eps: {REAL_EXPLORATION_FINAL_EPS} (ë‚®ì¶¤)")
        print(f"  Replay Buffer: ìœ ì§€ (ì‹œë®¬ë ˆì´ì…˜ ê²½í—˜ í™œìš©)")
    except Exception as e:
        print(f"[ERROR] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # 4. ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    print("\n[4/5] ì½œë°± ì„¤ì • ì¤‘...")
    checkpoint_callback = CheckpointCallback(
        save_freq=2_000,  # 2Kë§ˆë‹¤ ì €ì¥
        save_path=REAL_CHECKPOINT_DIR,
        name_prefix="dqn_v2_real"
    )
    print("[OK] ì½œë°± ì„¤ì • ì™„ë£Œ")
    
    # 5. Fine-tuning í•™ìŠµ
    print("\n[5/5] Fine-tuning ì‹œì‘...")
    print("-" * 80)
    try:
        model.learn(
            total_timesteps=REAL_TIMESTEPS,
            callback=checkpoint_callback,
            progress_bar=True,
            log_interval=50,
            reset_num_timesteps=False  # íƒ€ì„ìŠ¤í… ì¹´ìš´íŠ¸ ì´ì–´ì„œ (200Kë¶€í„° ì‹œì‘)
        )
        print("\n[OK] Fine-tuning ì™„ë£Œ")
    except Exception as e:
        print(f"\n[ERROR] Fine-tuning ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    
    # 6. ìµœì¢… ëª¨ë¸ ì €ì¥
    print(f"\n[6/6] ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘: {REAL_MODEL_PATH}")
    model.save(REAL_MODEL_PATH)
    print("[OK] ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    env.close()
    
    print("\n" + "=" * 80)
    print(" Phase B: Fine-tuning ì™„ë£Œ!")
    print("=" * 80)
    
    return model


def train_dqn_v2_full():
    """
    DQN v2 ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    Phase A (Sim) + Phase B (Real)
    """
    print("\n")
    print("=" * 80)
    print(" " * 20 + "DQN v2 í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    print(f"\nì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì´ ì˜ˆìƒ ì‹œê°„: 3-5ì‹œê°„")
    print(f"ì´ íƒ€ì„ìŠ¤í…: {SIM_TIMESTEPS + REAL_TIMESTEPS:,}")
    print("\n")
    
    start_time = datetime.now()
    
    # Phase A: ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ
    phase_a_start = datetime.now()
    model_sim = train_phase_a_simulation()
    phase_a_duration = (datetime.now() - phase_a_start).total_seconds() / 60
    
    if model_sim is None:
        print("\n[FAILED] Phase A ì‹¤íŒ¨. í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    print(f"\n[SUCCESS] Phase A ì™„ë£Œ (ì†Œìš” ì‹œê°„: {phase_a_duration:.1f}ë¶„)")
    
    # Phase B: ì‹¤ì œ DB Fine-tuning
    phase_b_start = datetime.now()
    model_final = train_phase_b_finetuning()
    phase_b_duration = (datetime.now() - phase_b_start).total_seconds() / 60
    
    if model_final is None:
        print("\n[FAILED] Phase B ì‹¤íŒ¨. í•˜ì§€ë§Œ Phase A ëª¨ë¸ì€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return
    
    print(f"\n[SUCCESS] Phase B ì™„ë£Œ (ì†Œìš” ì‹œê°„: {phase_b_duration:.1f}ë¶„)")
    
    # ì „ì²´ ì™„ë£Œ
    total_duration = (datetime.now() - start_time).total_seconds() / 60
    
    print("\n\n")
    print("=" * 80)
    print(" " * 22 + "DQN v2 í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_duration:.1f}ë¶„ ({total_duration/60:.1f}ì‹œê°„)")
    print("\nğŸ“Š í•™ìŠµ ê²°ê³¼:")
    print(f"  - Phase A (Sim): {phase_a_duration:.1f}ë¶„, {SIM_TIMESTEPS:,} íƒ€ì„ìŠ¤í…")
    print(f"  - Phase B (Real): {phase_b_duration:.1f}ë¶„, {REAL_TIMESTEPS:,} íƒ€ì„ìŠ¤í…")
    print(f"\nğŸ’¾ ì €ì¥ëœ ëª¨ë¸:")
    print(f"  - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸: {SIM_MODEL_PATH}")
    print(f"  - ìµœì¢… ëª¨ë¸: {REAL_MODEL_PATH}")
    print(f"\nğŸ“ˆ TensorBoard ë¡œê·¸: {TB_LOG_DIR}")
    print(f"   ì‹¤í–‰: tensorboard --logdir={TB_LOG_DIR}")
    print("\n[NEXT] ë‹¤ìŒ ë‹¨ê³„: v2_evaluate.pyë¡œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”.")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='DQN v2 í•˜ì´ë¸Œë¦¬ë“œ í•™ìŠµ')
    parser.add_argument('--phase', type=str, choices=['SimulXGB', 'RealDB', 'full'], default='full',
                        help='í•™ìŠµ ë‹¨ê³„: SimulXGB(ì‹œë®¬ë ˆì´ì…˜ë§Œ), RealDB(Fine-tuningë§Œ), full(ì „ì²´)')
    args = parser.parse_args()
    
    if args.phase == 'SimulXGB':
        train_phase_a_simulation()
    elif args.phase == 'RealDB':
        train_phase_b_finetuning()
    else:
        train_dqn_v2_full()

