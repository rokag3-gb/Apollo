# 🤖 RLQO 모델 비교 및 앙상블 전략

**작성일**: 2025-10-28  
**대상 모델**: DQN v4, PPO v3, DDPG v1, SAC v1  
**목적**: 단일 모델의 한계를 파악하고 앙상블의 필요성 제시

---

## 📊 모델별 핵심 특징 비교

| 특징 | DQN v4 | PPO v3 | DDPG v1 | SAC v1 |
|------|--------|--------|---------|--------|
| **알고리즘 타입** | Value-based | Policy Gradient | Actor-Critic | Maximum Entropy |
| **액션 공간** | 이산형 (19개) | 이산형 (8개) | 연속형 (7차원) | 연속형 (7차원) |
| **학습 방식** | Sim-to-Real | Sim-to-Real | Sim-to-Real | Sim-to-Real |
| **정책 타입** | Deterministic | Stochastic | Deterministic | Stochastic |
| **주요 강점** | 극단적 최적화 | 균형잡힌 성능 | 안전성 | 최고 성능 |
| **주요 약점** | CTE 취약 | 보수적 | 매우 보수적 | 변동성 높음 |
| **특화 영역** | 단순 쿼리 | CTE 쿼리 | 대용량 스캔 | 대용량 스캔 |

---

## 🎯 성능 지표 비교

### 전체 성능 요약

| 지표 | DQN v4 | PPO v3 | DDPG v1 | SAC v1 |
|------|--------|--------|---------|--------|
| **평균 Speedup** | **inf** | **1.199x** | **1.875x** | **1.891x** |
| **평균 개선율** | -597% | **+19.9%** | **+87.5%** | **+89.1%** |
| **최대 Speedup** | inf (0ms) | 76.0x | 18.1x | **22.1x** |
| **Win Rate** | 96.7% | - | - | 15.4% |
| **적용 쿼리 수** | 15/30 (50%) | 9/30 (31%) | 4/30 (13%) | 7/30 (23%) |
| **NO_ACTION 비율** | 50% | **69%** | **83%** | **84%** |
| **성능 저하 건수** | 6개 (심각) | 소수 | **0건** | **0건** |

**해석**:
- **DQN v4**: 극단적 (일부 0ms, 일부 1초+)
- **PPO v3**: 안정적이고 균형잡힘
- **DDPG v1**: 매우 안전하고 보수적
- **SAC v1**: 최고 성능이지만 선택적

---

## 🔬 쿼리 타입별 강점 분석

### CTE (Common Table Expression) 쿼리

| 모델 | Query 1 | Query 8 | Query 11 | 평균 | 평가 |
|------|---------|---------|----------|------|------|
| **DQN v4** | 0.025x ❌ | 0.54x ⚠️ | 0.029x ❌ | 0.20x | **최악** |
| **PPO v3** | **4.10x** ✅ | 1.23x ✅ | 1.07x ✅ | **1.70x** | **최고** 🥇 |
| **DDPG v1** | 1.0x ➡️ | 1.0x ➡️ | 1.07x ✅ | 1.02x | 안전 |
| **SAC v1** | 1.0x ➡️ | 1.0x ➡️ | 1.0x ➡️ | 1.0x | NO_ACTION |

**결론**: **PPO v3가 CTE 쿼리에 압도적으로 강함** (70% 향상)

### 대용량 테이블 스캔 (Query 2)

| 모델 | Speedup | 개선율 | 적용 액션 | 평가 |
|------|---------|--------|----------|------|
| **DQN v4** | 0.92x | -9% | Action 4 | ⚠️ 악화 |
| **PPO v3** | 1.0x | 0% | NO_ACTION | ➡️ 유지 |
| **DDPG v1** | **17.8x** | **+1682%** | MAXDOP=1, FAST=100, JOIN=FORCE_ORDER | 🥇 최고 |
| **SAC v1** | **17.9x** | **+1694%** | MAXDOP=1, FAST=100, JOIN=FORCE_ORDER + 추가 힌트 | 🥇 최고 |

**결론**: **DDPG v1과 SAC v1이 대용량 스캔에 특화**

### NOT EXISTS 서브쿼리 (Query 5)

| 모델 | Speedup | 개선율 | 적용 액션 | 평가 |
|------|---------|--------|----------|------|
| **DQN v4** | **inf** | **+100%** | Action 4 | 🥇 초고속 (0ms) |
| **PPO v3** | 1.0x | 0% | NO_ACTION | ➡️ 유지 |
| **DDPG v1** | **5.85x** | **+485%** | MAXDOP=1, FAST=100 | ✅ 우수 |
| **SAC v1** | **5.94x** | **+494%** | MAXDOP=1, FAST=20, JOIN=MERGE | ✅ 우수 |

**결론**: **DQN v4가 서브쿼리에 극적으로 강함**

### 복잡한 조인 (Query 0, 4, 14, 24)

| 모델 | 평균 Speedup | 성능 저하 | 평가 |
|------|--------------|----------|------|
| **DQN v4** | 0.33x | **4개 모두 악화** | ❌ 최악 |
| **PPO v3** | 1.20x | 없음 | ✅ 우수 |
| **DDPG v1** | 1.0x | 없음 | ➡️ 안전 |
| **SAC v1** | 1.0x | 없음 | ➡️ 안전 |

**결론**: **DQN v4는 복잡한 조인에 취약, PPO v3는 안정적**

---

## 🎭 모델별 강점/약점 매트릭스

### 강점 (✅ Excellent)

| 쿼리 타입 | DQN v4 | PPO v3 | DDPG v1 | SAC v1 |
|----------|--------|--------|---------|--------|
| **CTE 쿼리** | ❌ | ✅✅✅ | ➡️ | ➡️ |
| **대용량 스캔** | ❌ | ➡️ | ✅✅✅ | ✅✅✅ |
| **서브쿼리** | ✅✅✅ | ➡️ | ✅✅ | ✅✅ |
| **단순 필터링** | ✅✅✅ | ✅ | ➡️ | ➡️ |
| **복잡한 조인** | ❌❌ | ✅✅ | ➡️ | ➡️ |
| **윈도우 함수** | ⚠️ | ✅ | ➡️ | ➡️ |
| **집계 쿼리** | ⚠️ | ✅ | ➡️ | ➡️ |

### 약점 분석

| 모델 | 주요 약점 | 심각도 | 영향 쿼리 수 |
|------|----------|--------|-------------|
| **DQN v4** | CTE 및 복잡한 조인에서 치명적 악화 (1초+) | ⚠️⚠️⚠️ **심각** | 6/30 (20%) |
| **PPO v3** | 보수적 접근 (69% NO_ACTION) | ⚠️ 보통 | - |
| **DDPG v1** | 매우 보수적 (83% NO_ACTION) | ⚠️ 보통 | - |
| **SAC v1** | 높은 변동성, 예측 어려움 | ⚠️ 보통 | - |

---

## 🤝 왜 앙상블이 필요한가?

### 1️⃣ 단일 모델의 한계

#### DQN v4의 딜레마
```
✅ 15개 쿼리에서 극적 개선 (0ms ~ 26x)
❌ 6개 쿼리에서 치명적 악화 (865ms ~ 1,032ms)
📊 순 효과: 전체적으로 느려짐 (-597%)
```

#### PPO v3의 한계
```
✅ 안정적이고 균형잡힘 (+19.9%)
⚠️ 69% NO_ACTION → 보수적
📊 개선 여지가 많이 남음
```

#### DDPG v1 & SAC v1의 한계
```
✅ 대용량 스캔에 특화 (18~22x)
⚠️ 83~84% NO_ACTION → 매우 보수적
📊 대부분 쿼리에서 적용 안 함
```

### 2️⃣ 상호 보완적 특성

| 시나리오 | 최적 모델 | 이유 |
|----------|----------|------|
| **CTE 포함 복잡한 쿼리** | PPO v3 | 70% 향상, 안정적 |
| **대용량 테이블 스캔** | SAC v1 or DDPG v1 | 18~22x 개선 |
| **단순 서브쿼리** | DQN v4 | 0ms 초고속 달성 |
| **복잡한 5-way JOIN** | PPO v3 | DQN v4는 악화 위험 |
| **위험 회피 필요** | DDPG v1 | 성능 저하 0건 |

### 3️⃣ 앙상블의 기대 효과

#### 시나리오 1: Voting 앙상블 (4개 모델)
```python
Query 1 (CTE):
  - DQN v4:  987ms (❌ 악화)
  - PPO v3:  6ms   (✅ 대폭 개선, 4.10x)
  - DDPG v1: 25ms  (➡️ 유지)
  - SAC v1:  25ms  (➡️ 유지)
  
→ Majority Voting: PPO v3 + DDPG v1 + SAC v1 = NO_ACTION 또는 PPO 액션
→ 결과: 안전하게 개선 또는 유지
```

#### 시나리오 2: 쿼리 타입별 라우팅
```python
if query_has_CTE():
    use PPO_v3  # 70% 향상
elif query_is_large_scan():
    use SAC_v1  # 22x 개선
elif query_is_simple_subquery():
    use DQN_v4  # 0ms 달성
else:
    use DDPG_v1  # 가장 안전
```

---

## 📈 앙상블 성능 시뮬레이션

### 단일 모델 vs 앙상블 비교 (이론적 추정)

| 지표 | 최고 단일 모델 | 앙상블 (예상) | 개선 |
|------|---------------|--------------|------|
| **평균 Speedup** | 1.891x (SAC v1) | **2.5~3.0x** | +32~59% |
| **성능 저하 건수** | 0건 (DDPG/SAC) | **0건** | 유지 |
| **적용 쿼리 수** | 7/30 (SAC v1) | **18~22/30** | +157~214% |
| **극적 개선 (10x+)** | 5/30 | **8~10/30** | +60~100% |
| **안정성** | 높음 | **매우 높음** | ✅ |

### 기대 효과

1. **DQN v4의 강점 활용 + 약점 제거**
   - ✅ 15개 쿼리 극적 개선 유지
   - ✅ 6개 악화 쿼리는 다른 모델로 대체
   
2. **PPO v3의 CTE 최적화 활용**
   - ✅ CTE 쿼리 3개에서 70% 향상
   
3. **DDPG/SAC의 대용량 최적화 활용**
   - ✅ 대용량 스캔 쿼리에서 18~22x 개선
   
4. **안전성 극대화**
   - ✅ 4개 모델의 합의로 위험한 액션 회피
   - ✅ 성능 저하 최소화

---

## 🎯 앙상블 전략 제안

### Strategy 1: Majority Voting (다수결)
```
4개 모델 중 3개 이상 동의하는 액션만 적용
→ 안전성 최우선
→ 보수적이지만 신뢰성 높음
```

**장점**: 성능 저하 위험 최소화  
**단점**: 공격적 최적화 기회 감소

### Strategy 2: Performance-Weighted Voting (성능 가중치)
```
각 쿼리 타입별로 최고 성능 모델에 더 높은 가중치
- CTE: PPO v3 (가중치 0.5)
- 대용량: SAC v1 (가중치 0.5)
- 서브쿼리: DQN v4 (가중치 0.5)
```

**장점**: 각 모델의 강점 극대화  
**단점**: 가중치 조정 필요

### Strategy 3: Query-Type Router (쿼리 타입 라우팅)
```
쿼리 분석 → 최적 모델 선택
- CTE 감지 → PPO v3
- 대용량 스캔 → SAC v1
- 단순 서브쿼리 → DQN v4
- 기타 → DDPG v1 (가장 안전)
```

**장점**: 최대 성능  
**단점**: 쿼리 분석기 필요

### Strategy 4: Confidence-Based Selection (신뢰도 기반)
```
각 모델의 예측 신뢰도(Q-value, policy entropy)를 비교
→ 가장 확신하는 모델의 액션 선택
```

**장점**: 동적이고 적응적  
**단점**: 복잡한 구현

---

## 📊 실제 Ensemble v1 결과 (예시)

### 구현된 전략
- **Equal Voting**: 4개 모델 동등 투표
- **Majority Voting**: 3/4 이상 동의
- **Performance-Weighted**: 성능 기반 가중치
- **Query-Type**: 쿼리 타입별 최적 모델

### 초기 결과 (진행 중)
```
✅ CTE 쿼리: PPO v3 적용으로 70% 향상 달성
✅ 대용량 스캔: SAC v1/DDPG v1 적용으로 18~22x 개선
✅ 성능 저하: 거의 제거 (앙상블 효과)
⚠️ 추가 튜닝: 가중치 최적화 진행 중
```

---

## 💡 결론 및 권고사항

### 핵심 메시지

> **"하나의 모델로는 모든 쿼리를 최적화할 수 없다"**
> 
> - DQN v4: 극단적 성능, CTE 취약
> - PPO v3: 균형잡힘, CTE 강점
> - DDPG v1: 매우 안전, 대용량 특화
> - SAC v1: 최고 성능, 대용량 특화
>
> **→ 앙상블로 각 모델의 강점을 결합하고 약점을 상쇄**

### 권고사항

1. ✅ **Ensemble v1 배포 권장**
   - Majority Voting으로 안전성 확보
   - Query-Type Router로 성능 극대화

2. ✅ **단일 모델 사용 지양**
   - DQN v4 단독 사용 금지 (CTE 위험)
   - PPO v3만으로는 기회 손실

3. ✅ **지속적 모니터링**
   - 쿼리 타입별 성능 추적
   - 앙상블 가중치 튜닝

4. ✅ **점진적 적용**
   - 안전성 높은 쿼리부터 시작
   - 프로덕션 투입 전 충분한 검증

---

## 📚 참고 자료

- **DQN v4 평가 보고서**: `Apollo.ML/RLQO/DQN_v4/DQN_v4_Evaluation_Report.md`
- **PPO v3 평가 보고서**: `Apollo.ML/RLQO/PPO_v3/PPO_v3_Evaluation_Report.md`
- **DDPG v1 평가 보고서**: `Apollo.ML/RLQO/DDPG_v1/DDPG_v1_Evaluation_Report.md`
- **SAC v1 평가 보고서**: `Apollo.ML/RLQO/SAC_v1/SAC_v1_Evaluation_Report.md`
- **Ensemble v1 보고서**: `Apollo.ML/RLQO/Ensemble_v1/Ensemble_Evaluation_Report.md`

---

**작성**: Apollo ML Team  
**최종 업데이트**: 2025-10-28  
**버전**: 1.0

