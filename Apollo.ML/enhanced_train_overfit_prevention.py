# -*- coding: utf-8 -*-
"""
Phase 2: ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ + Overfitting ë°©ì§€
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings('ignore')

def phase2_overfit_prevention():
    """Phase 2: ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ + Overfitting ë°©ì§€"""
    
    print("=== Phase 2: ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ + Overfitting ë°©ì§€ ===")
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = pd.read_parquet("artifacts/enhanced_features.parquet")
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    
    # 2. ì‹¤í–‰ê³„íš êµ¬ì¡° ë¶„ì„ ê°•í™” (íƒ€ê²Ÿ ë³€ìˆ˜ ì •ë³´ ëˆ„ì¶œ ë°©ì§€)
    print("\n1. ì‹¤í–‰ê³„íš êµ¬ì¡° ë¶„ì„ ê°•í™”...")
    df = enhanced_plan_analysis_safe(df)
    
    # 3. ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€ (ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©)
    print("\n2. ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€ (ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©)...")
    df = add_temporal_features_safe(df)
    
    # 4. ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜
    print("\n3. ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜...")
    df = add_domain_features(df)
    
    # 5. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ í”¼ì²˜ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
    print("\n4. í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ í”¼ì²˜ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)...")
    df = add_clustering_features_safe(df)
    
    # 6. í”¼ì²˜ ì„ íƒ ë° ì •ê·œí™”
    print("\n5. í”¼ì²˜ ì„ íƒ ë° ì •ê·œí™”...")
    df = feature_selection_and_scaling(df)
    
    # 7. ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ (Overfitting ë°©ì§€)
    print("\n6. Overfitting ë°©ì§€ ëª¨ë¸ í›ˆë ¨...")
    results = train_overfit_prevention_model(df)
    
    return results

def enhanced_plan_analysis_safe(df):
    """íƒ€ê²Ÿ ë³€ìˆ˜ ì •ë³´ ëˆ„ì¶œì„ ë°©ì§€í•œ ì‹¤í–‰ê³„íš êµ¬ì¡° ë¶„ì„"""
    
    # ì‹¤í–‰ê³„íš XMLì—ì„œ ì¶”ê°€ íŠ¹ì„± ì¶”ì¶œ (íƒ€ê²Ÿ ë³€ìˆ˜ì™€ ë¬´ê´€í•œ êµ¬ì¡°ì  íŠ¹ì„±ë§Œ)
    plan_features = []
    
    for idx, row in df.iterrows():
        if idx % 1000 == 0:
            print(f"  ì‹¤í–‰ê³„íš ë¶„ì„ ì¤‘: {idx + 1}/{len(df)}")
        
        try:
            from plan_graph import planxml_to_graph
            g = planxml_to_graph(row["plan_xml"])
            
            # 1. ì‹¤í–‰ê³„íš íŠ¸ë¦¬ ê¹Šì´ (êµ¬ì¡°ì  íŠ¹ì„±)
            if g.number_of_nodes() > 0:
                longest_path = 0
                for node in g.nodes():
                    if g.in_degree(node) == 0:  # ë£¨íŠ¸ ë…¸ë“œ
                        path_length = nx.single_source_shortest_path_length(g, node)
                        longest_path = max(longest_path, max(path_length.values()) if path_length else 0)
                
                plan_features.append({
                    'plan_id': row['plan_id'],
                    'tree_depth': longest_path,
                    'max_parallel_levels': calculate_max_parallel_levels(g),
                    'join_complexity': calculate_join_complexity(g),
                    'index_usage_score': calculate_index_usage_score(g),
                    'memory_intensity': calculate_memory_intensity(g),
                    'operator_diversity': calculate_operator_diversity(g)
                })
            else:
                plan_features.append({
                    'plan_id': row['plan_id'],
                    'tree_depth': 0,
                    'max_parallel_levels': 0,
                    'join_complexity': 0,
                    'index_usage_score': 0,
                    'memory_intensity': 0,
                    'operator_diversity': 0
                })
                
        except Exception as e:
            plan_features.append({
                'plan_id': row['plan_id'],
                'tree_depth': 0,
                'max_parallel_levels': 0,
                'join_complexity': 0,
                'index_usage_score': 0,
                'memory_intensity': 0,
                'operator_diversity': 0
            })
    
    # í”¼ì²˜ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    plan_df = pd.DataFrame(plan_features)
    
    # ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•©
    df = df.merge(plan_df, on='plan_id', how='left')
    
    return df

def calculate_max_parallel_levels(g):
    """ìµœëŒ€ ë³‘ë ¬ ì²˜ë¦¬ ë ˆë²¨ ê³„ì‚°"""
    parallel_levels = 0
    for node, attrs in g.nodes(data=True):
        if attrs.get('Parallel', False):
            parallel_levels += 1
    return parallel_levels

def calculate_join_complexity(g):
    """ì¡°ì¸ ë³µì¡ë„ ê³„ì‚°"""
    join_ops = 0
    for node, attrs in g.nodes(data=True):
        if 'PhysicalOp' in attrs:
            op = attrs['PhysicalOp'].lower()
            if 'join' in op or 'merge' in op or 'hash' in op:
                join_ops += 1
    return join_ops

def calculate_index_usage_score(g):
    """ì¸ë±ìŠ¤ ì‚¬ìš© ì ìˆ˜ ê³„ì‚°"""
    index_ops = 0
    total_ops = g.number_of_nodes()
    
    for node, attrs in g.nodes(data=True):
        if 'PhysicalOp' in attrs:
            op = attrs['PhysicalOp'].lower()
            if 'scan' in op or 'seek' in op:
                index_ops += 1
    
    return index_ops / total_ops if total_ops > 0 else 0

def calculate_memory_intensity(g):
    """ë©”ëª¨ë¦¬ ì§‘ì•½ë„ ê³„ì‚°"""
    memory_ops = 0
    for node, attrs in g.nodes(data=True):
        if 'PhysicalOp' in attrs:
            op = attrs['PhysicalOp'].lower()
            if 'sort' in op or 'hash' in op or 'spool' in op:
                memory_ops += 1
    return memory_ops

def calculate_operator_diversity(g):
    """ì—°ì‚°ì ë‹¤ì–‘ì„± ê³„ì‚°"""
    unique_ops = set()
    for node, attrs in g.nodes(data=True):
        if 'PhysicalOp' in attrs:
            unique_ops.add(attrs['PhysicalOp'])
    return len(unique_ops)

def add_temporal_features_safe(df):
    """ê³¼ê±° ì •ë³´ë§Œ ì‚¬ìš©í•œ ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€"""
    
    # 1. ì‹œê°„ëŒ€ë³„ íŠ¹ì„± (ê³¼ê±° ì •ë³´)
    if 'last_exec_time' in df.columns:
        df['last_exec_time'] = pd.to_datetime(df['last_exec_time'])
        df['hour_of_day'] = df['last_exec_time'].dt.hour
        df['day_of_week'] = df['last_exec_time'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 18)).astype(int)
    
    # 2. ì¿¼ë¦¬ ì‹¤í–‰ ë¹ˆë„ íŠ¹ì„± (ê³¼ê±° ì •ë³´)
    if 'query_id' in df.columns:
        query_counts = df['query_id'].value_counts()
        df['query_frequency'] = df['query_id'].map(query_counts)
        df['is_high_frequency'] = (df['query_frequency'] > query_counts.quantile(0.8)).astype(int)
    
    # 3. ì„±ëŠ¥ íŠ¸ë Œë“œ íŠ¹ì„± (ê³¼ê±° í‰ê·  ëŒ€ë¹„)
    if 'avg_ms' in df.columns and 'last_ms' in df.columns:
        df['performance_trend'] = df['last_ms'] / (df['avg_ms'] + 1e-6)
        df['is_performance_degrading'] = (df['performance_trend'] > 1.5).astype(int)
    
    return df

def add_domain_features(df):
    """ë„ë©”ì¸ íŠ¹í™” í”¼ì²˜ ì¶”ê°€"""
    
    # 1. ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± í”¼ì²˜
    if 'last_cpu_ms' in df.columns and 'last_ms' in df.columns:
        df['cpu_efficiency'] = df['last_cpu_ms'] / (df['last_ms'] + 1e-6)
    
    if 'last_reads' in df.columns and 'last_ms' in df.columns:
        df['io_efficiency'] = df['last_reads'] / (df['last_ms'] + 1e-6)
    
    # 2. ë³µì¡ë„ ì ìˆ˜
    complexity_features = ['num_nodes', 'num_edges', 'num_logical_ops', 'num_physical_ops']
    available_complexity = [col for col in complexity_features if col in df.columns]
    
    if available_complexity:
        df['complexity_score'] = df[available_complexity].sum(axis=1)
        df['normalized_complexity'] = df['complexity_score'] / df['complexity_score'].max()
    
    # 3. ë¦¬ì†ŒìŠ¤ ì§‘ì•½ë„
    resource_features = ['max_used_mem_kb', 'last_cpu_ms', 'last_reads']
    available_resources = [col for col in resource_features if col in df.columns]
    
    if available_resources:
        df['resource_intensity'] = df[available_resources].sum(axis=1)
        df['is_resource_intensive'] = (df['resource_intensity'] > df['resource_intensity'].quantile(0.8)).astype(int)
    
    # 4. ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„±
    if 'max_dop' in df.columns and 'last_ms' in df.columns:
        df['parallel_efficiency'] = df['max_dop'] / (df['last_ms'] + 1e-6)
        df['is_parallel_efficient'] = (df['parallel_efficiency'] > df['parallel_efficiency'].quantile(0.7)).astype(int)
    
    return df

def add_clustering_features_safe(df):
    """íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸í•œ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ í”¼ì²˜ ì¶”ê°€"""
    
    # í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  í”¼ì²˜ ì„ íƒ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
    clustering_features = [
        'num_nodes', 'num_edges', 'num_logical_ops', 'num_physical_ops',
        'total_estimated_cost', 'avg_ms', 'last_cpu_ms', 'last_reads',
        'max_used_mem_kb', 'max_dop', 'tree_depth', 'join_complexity',
        'index_usage_score', 'memory_intensity', 'operator_diversity'
    ]
    
    available_features = [col for col in clustering_features if col in df.columns]
    
    if len(available_features) >= 3:
        # ë°ì´í„° ì •ê·œí™”
        scaler = StandardScaler()
        X_cluster = scaler.fit_transform(df[available_features].fillna(0))
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        df['query_cluster'] = kmeans.fit_predict(X_cluster)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
        cluster_stats = df.groupby('query_cluster')[available_features].agg(['mean', 'std']).reset_index()
        
        # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì¶”ê°€
        for feature in available_features:
            df[f'cluster_{feature}_mean'] = df['query_cluster'].map(cluster_stats.set_index('query_cluster')[feature]['mean'])
            df[f'cluster_{feature}_std'] = df['query_cluster'].map(cluster_stats.set_index('query_cluster')[feature]['std'])
    
    return df

def feature_selection_and_scaling(df):
    """í”¼ì²˜ ì„ íƒ ë° ì •ê·œí™”"""
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [col for col in numeric_cols if col not in ['plan_id', 'query_id', 'last_ms']]
    
    X = df[feature_cols].fillna(0)
    y = df['last_ms']
    
    # 1. ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì²˜ë“¤ ì œê±°
    corr_matrix = X.corr().abs()
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]
    
    print(f"  ê³ ìƒê´€ê´€ê³„ í”¼ì²˜ ì œê±°: {len(high_corr_features)}ê°œ")
    feature_cols = [col for col in feature_cols if col not in high_corr_features]
    
    # 2. SelectKBestë¡œ ìƒìœ„ í”¼ì²˜ ì„ íƒ
    if len(feature_cols) > 25:  # í”¼ì²˜ê°€ ë§ì„ ë•Œë§Œ ì„ íƒ
        selector = SelectKBest(f_regression, k=25)
        X_selected = selector.fit_transform(X[feature_cols], y)
        selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]
        print(f"  SelectKBest ì„ íƒëœ í”¼ì²˜: {len(selected_features)}ê°œ")
    else:
        selected_features = feature_cols
    
    # 3. RFEë¡œ ì¶”ê°€ í”¼ì²˜ ì„ íƒ
    if len(selected_features) > 15:
        xgb = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        rfe = RFE(estimator=xgb, n_features_to_select=15)
        X_rfe = rfe.fit_transform(X[selected_features], y)
        final_features = [selected_features[i] for i in rfe.get_support(indices=True)]
        print(f"  RFE ì„ íƒëœ í”¼ì²˜: {len(final_features)}ê°œ")
    else:
        final_features = selected_features
    
    # ì„ íƒëœ í”¼ì²˜ë§Œ ìœ ì§€
    df_selected = df[final_features + ['plan_id', 'query_id', 'last_ms']].copy()
    
    return df_selected

def train_overfit_prevention_model(df):
    """Overfitting ë°©ì§€ ëª¨ë¸ í›ˆë ¨"""
    
    # í”¼ì²˜ ì„ íƒ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = [col for col in numeric_cols if col not in ['plan_id', 'query_id', 'last_ms']]
    
    X = df[feature_cols]
    y = df['last_ms']
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # Overfitting ë°©ì§€ ëª¨ë¸ ì„¤ì •
    model = XGBRegressor(
        n_estimators=300,  # ê°ì†Œ
        max_depth=5,       # ê°ì†Œ
        learning_rate=0.05,  # ê°ì†Œ
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.2,     # L1 ì •ê·œí™” ê°•í™”
        reg_lambda=0.2,    # L2 ì •ê·œí™” ê°•í™”
        min_child_weight=10,  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì œí•œ ê°•í™”
        random_state=42,
        n_jobs=-1
    )
    
    # í›ˆë ¨
    model.fit(X_train_scaled, y_train)
    
    # ì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í‰ê°€ìš©)
    X_scaled = scaler.transform(X)
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    y_pred = model.predict(X_val_scaled)
    
    # ë‹¤ì–‘í•œ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    mae = mean_absolute_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    
    # MAPE ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    mape = np.mean(np.abs((y_val - y_pred) / np.maximum(y_val, 1e-8))) * 100
    
    print(f"\n  === ê²€ì¦ ë°ì´í„° í‰ê°€ ê²°ê³¼ ===")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  MAE: {mae:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  MAPE: {mape:.2f}%")
    
    # ìƒì„¸ ë¶„ì„
    print(f"\n  === ìƒì„¸ ë¶„ì„ ===")
    print(f"  ì‹¤ì œê°’ ë²”ìœ„: {y_val.min():.2f} ~ {y_val.max():.2f}")
    print(f"  ì˜ˆì¸¡ê°’ ë²”ìœ„: {y_pred.min():.2f} ~ {y_pred.max():.2f}")
    print(f"  ì‹¤ì œê°’ í‰ê· : {y_val.mean():.2f}")
    print(f"  ì˜ˆì¸¡ê°’ í‰ê· : {y_pred.mean():.2f}")
    
    # ì˜¤ì°¨ ë¶„ì„
    residuals = y_val - y_pred
    print(f"  ì”ì°¨ í‰ê· : {residuals.mean():.2f}")
    print(f"  ì”ì°¨ í‘œì¤€í¸ì°¨: {residuals.std():.2f}")
    
    # ì„±ëŠ¥ êµ¬ê°„ë³„ ë¶„ì„
    print(f"\n  === ì„±ëŠ¥ êµ¬ê°„ë³„ ë¶„ì„ ===")
    for threshold in [1, 10, 100, 1000]:
        mask = y_val < threshold
        if mask.sum() > 0:
            subset_r2 = r2_score(y_val[mask], y_pred[mask])
            print(f"  {threshold}ms ë¯¸ë§Œ ì¿¼ë¦¬ RÂ²: {subset_r2:.4f} (ìƒ˜í”Œ ìˆ˜: {mask.sum()})")
    
    # êµì°¨ ê²€ì¦ìœ¼ë¡œ overfitting í™•ì¸
    print("\n  êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
    print(f"  êµì°¨ ê²€ì¦ RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    # í›ˆë ¨/ê²€ì¦ ì„±ëŠ¥ ì°¨ì´ í™•ì¸
    train_r2 = model.score(X_train_scaled, y_train)
    val_r2 = model.score(X_val_scaled, y_val)
    print(f"  í›ˆë ¨ RÂ²: {train_r2:.4f}, ê²€ì¦ RÂ²: {val_r2:.4f}")
    print(f"  ì„±ëŠ¥ ì°¨ì´: {abs(train_r2 - val_r2):.4f}")
    
    # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\n  ìƒìœ„ 10ê°œ í”¼ì²˜ ì¤‘ìš”ë„:")
    print(feature_importance.head(10))
    
    return {
        'rmse': rmse,
        'r2': r2,
        'cv_r2_mean': cv_scores.mean(),
        'cv_r2_std': cv_scores.std(),
        'train_r2': train_r2,
        'val_r2': val_r2,
        'overfitting_gap': abs(train_r2 - val_r2),
        'model': model,
        'scaler': scaler,
        'feature_importance': feature_importance,
        'X_processed': pd.DataFrame(X_scaled, columns=feature_cols),
        'y_processed': pd.Series(y, name='last_ms')
    }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    results = phase2_overfit_prevention()
    
    print(f"\n=== Phase 2 Overfitting ë°©ì§€ ìµœì¢… ê²°ê³¼ ===")
    print(f"ê²€ì¦ RMSE: {results['rmse']:.2f}")
    print(f"ê²€ì¦ RÂ²: {results['r2']:.4f}")
    print(f"êµì°¨ ê²€ì¦ RÂ²: {results['cv_r2_mean']:.4f} Â± {results['cv_r2_std']:.4f}")
    print(f"í›ˆë ¨ RÂ²: {results['train_r2']:.4f}")
    print(f"ê²€ì¦ RÂ²: {results['val_r2']:.4f}")
    print(f"Overfitting Gap: {results['overfitting_gap']:.4f}")
    
    # ëª¨ë¸ í’ˆì§ˆ í‰ê°€
    print(f"\n=== ëª¨ë¸ í’ˆì§ˆ í‰ê°€ ===")
    if results['r2'] >= 0.9:
        print("ğŸŸ¢ ìš°ìˆ˜í•œ ì„±ëŠ¥ (RÂ² â‰¥ 0.9)")
    elif results['r2'] >= 0.7:
        print("ğŸŸ¡ ì–‘í˜¸í•œ ì„±ëŠ¥ (0.7 â‰¤ RÂ² < 0.9)")
    elif results['r2'] >= 0.5:
        print("ğŸŸ  ë³´í†µ ì„±ëŠ¥ (0.5 â‰¤ RÂ² < 0.7)")
    else:
        print("ğŸ”´ ë‚®ì€ ì„±ëŠ¥ (RÂ² < 0.5)")
    
    # Overfitting íŒì •
    print(f"\n=== Overfitting ë¶„ì„ ===")
    if results['overfitting_gap'] < 0.05:
        print("âœ… Overfitting ì—†ìŒ (Gap < 0.05)")
    elif results['overfitting_gap'] < 0.1:
        print("âš ï¸  ê²½ë¯¸í•œ Overfitting (Gap < 0.1)")
    else:
        print("âŒ Overfitting ì˜ì‹¬ (Gap >= 0.1)")
    
    # ì•ˆì •ì„± í‰ê°€
    print(f"\n=== ëª¨ë¸ ì•ˆì •ì„± í‰ê°€ ===")
    if results['cv_r2_std'] < 0.01:
        print("ğŸŸ¢ ë§¤ìš° ì•ˆì •ì  (CV std < 0.01)")
    elif results['cv_r2_std'] < 0.05:
        print("ğŸŸ¡ ì•ˆì •ì  (CV std < 0.05)")
    else:
        print("ğŸŸ  ë¶ˆì•ˆì • (CV std â‰¥ 0.05)")
    
    # ëª¨ë¸ ì €ì¥
    import joblib
    joblib.dump(results['model'], 'artifacts/model.joblib')
    joblib.dump(results['scaler'], 'artifacts/scaler.joblib')
    results['feature_importance'].to_csv('artifacts/model_importance.csv', index=False)
    
    # ì²˜ë¦¬ëœ í”¼ì²˜ ë°ì´í„°ë„ ì €ì¥ (í‰ê°€ìš©)
    results['X_processed'].to_parquet('artifacts/processed_features.parquet', index=False)
    results['y_processed'].to_frame().to_parquet('artifacts/processed_target.parquet', index=False)
    
    print(f"\nëª¨ë¸ ì €ì¥ ì™„ë£Œ: artifacts/model.joblib")
    print(f"ì²˜ë¦¬ëœ í”¼ì²˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ: artifacts/processed_features.parquet")

if __name__ == "__main__":
    main()
