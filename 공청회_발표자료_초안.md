# 딥러닝 기반 SQL Server 쿼리 최적화 연구
## 공청회 발표자료 초안

---

## 목차

1. 연구 배경 및 목적 (5~7장)
2. 연구 내용 (14~16장)
3. 연구 결과 (6~8장)
4. 결론 (2~3장)

**총 슬라이드 수: 30~40장**

---

# 2. 연구 내용 (14~16장)

---

## 슬라이드 2-1: 연구 방법론 개요

### 내용
- 전체 연구 프로세스 플로우차트
  1. 가상 원장 DB 구축 (TradingDB)
  2. 실행계획 수집 및 전처리
  3. 지도학습 모델 (XGBoost)
  4. 강화학습 모델 (DQN, PPO, DDPG, SAC)
  5. 앙상블 모델
  6. 평가 및 최적화

### 필요한 이미지
- **플로우차트**: 전체 연구 파이프라인을 보여주는 다이어그램
- 각 단계별 입출력 데이터 형식

---

## 슬라이드 2-2: 가상 원장 DB 구축 - 설계 목적

### 내용
- **구축 목적**
  - 실제 운영 DB의 부하 없이 쿼리 최적화 실험 수행
  - 다양한 쿼리 패턴 생성 및 테스트 환경 제공
  - 재현 가능한 벤치마크 환경 구축

- **TradingDB 선택 이유**
  - 복잡한 비즈니스 로직 (주문, 체결, 포지션, 손익 계산)
  - 다양한 조인 패턴 (1:N, M:N 관계)
  - 시계열 데이터 포함
  - 실제 금융 서비스와 유사한 워크로드

### 필요한 이미지
- **TradingDB ERD**: 테이블 간 관계도
- 주요 테이블 목록 (44개 테이블)

---

## 슬라이드 2-3: 가상 원장 DB 구축 - 스키마 구성

### 내용
- **총 44개 테이블**
  1. **기준 데이터** (10개): 국가, 통화, 거래소, 종목 등
  2. **고객/계좌** (3개): 고객, 계좌, 마진 계좌
  3. **거래 원장** (6개): 주문, 체결, 포지션, 현금 원장 등
  4. **시장 데이터** (2개): 장중 시세, 종가 데이터
  5. **리스크/컴플라이언스** (4개): 리스크 노출, 컴플라이언스 룰 등
  6. **시스템** (5개): 사용자, 감사 로그, 배치 작업 등
  7. **분석 차원** (14개): 스타 스키마 기반 OLAP 구조

- **데이터 규모**
  - 약 수백만 ~ 수천만 건의 트랜잭션 데이터
  - 실시간 쿼리와 분석 쿼리 혼재

### 필요한 이미지
- **스키마 다이어그램**: 7개 카테고리별 테이블 분류
- **데이터 샘플**: 주요 테이블의 샘플 레코드

---

## 슬라이드 2-4: 실행계획 수집 시스템

### 내용
- **수집 대상 쿼리**
  - 다양한 복잡도의 SELECT 쿼리
  - 조인 수: 1개 ~ 8개 이상
  - 쿼리 유형: TOP, CTE, 서브쿼리, JOIN_HEAVY 등

- **수집 방법**
  - SQL Server의 실행계획 XML 추출
  - 저장 프로시저: `usp_CollectQueryPlans`
  - 수집 정보:
    - 쿼리 텍스트
    - 실행계획 XML
    - 실행 시간 (Baseline)
    - 논리적 읽기, 물리적 읽기
    - CPU 시간, 경과 시간

- **수집 결과**
  - 총 수집 쿼리 수: 수천 ~ 수만 건
  - Parquet 형식으로 저장 (`collected_plans.parquet`)

### 필요한 이미지
- **SQL Server Management Studio**: 실행계획 XML 예시
- **수집 프로세스 다이어그램**: DB → Python → Parquet
- **collected_plans.parquet 스키마**: 컬럼 목록 및 타입

---

## 슬라이드 2-5: 실행계획 전처리 - XML 파싱

### 내용
- **XML 파싱 과정**
  1. 실행계획 XML을 그래프 구조로 변환
  2. 각 노드(Operator)와 엣지(데이터 흐름) 추출
  3. 노드별 속성 추출:
     - PhysicalOp, LogicalOp
     - EstimateRows, ActualRows
     - EstimatedTotalSubtreeCost
     - 인덱스 정보 (Index Scan/Seek)
     - 조인 타입 (Nested Loop, Hash, Merge)

- **그래프 통계 특성 추출**
  - 노드 수, 엣지 수
  - 그래프 밀도
  - 최대 깊이
  - 평균 분기 계수

### 필요한 이미지
- **실행계획 XML 샘플**: 일부 노드 강조
- **그래프 변환 예시**: XML → 그래프 구조 다이어그램

---

## 슬라이드 2-6: 피처 엔지니어링

### 내용
- **추출된 피처 카테고리** (총 100개 이상)
  
  1. **그래프 구조 특성**
     - 노드 수, 엣지 수, 밀도, 깊이
  
  2. **비용 관련 특성**
     - 예상 총 비용, IO 비용, CPU 비용
  
  3. **연산자 특성**
     - 연산자 타입별 개수 (Scan, Seek, Join 등)
     - 병렬 처리 여부
  
  4. **인덱스 특성**
     - 클러스터드/논클러스터드 비율
     - Index Scan vs Seek 비율
  
  5. **데이터 흐름 특성**
     - 예상/실제 행 수
     - 추정 오차율

- **피처 정규화 및 스케일링**
  - StandardScaler 적용
  - 결측치 처리

### 필요한 이미지
- **피처 분포 히스토그램**: 주요 피처들의 분포
- **피처 상관관계 히트맵**: Correlation matrix

---

## 슬라이드 2-7: XGBoost 회귀 모델 학습

### 내용
- **모델 선택 이유**
  - 트리 기반 모델로 비선형 관계 학습 우수
  - 대용량 데이터셋에 효율적
  - 과적합 방지 (정규화, early stopping)
  - 빠른 학습 및 추론 속도

- **학습 설정**
  - Train/Test Split: 80% / 20%
  - 하이퍼파라미터:
    - max_depth: 10
    - n_estimators: 500
    - learning_rate: 0.1
    - early_stopping_rounds: 50

- **목표 변수**
  - 쿼리 실행 시간 (ms)
  - 로그 변환 적용 (스케일 조정)

### 필요한 이미지
- **학습 곡선**: Train/Validation Loss over iterations
- **하이퍼파라미터 튜닝 결과**: Grid search 결과표

---

## 슬라이드 2-8: XGBoost 모델 평가 및 피처 중요도

### 내용
- **평가 지표**
  - R² Score: **0.9955** (매우 높은 설명력)
  - RMSE: XX ms
  - MAE: XX ms

- **예측 정확도 분석**
  - 실제 vs 예측 시간 산점도
  - 대부분의 예측이 실제값에 근접
  - 일부 이상치 (복잡한 쿼리) 존재

- **피처 중요도 Top 10**
  - EstimatedTotalSubtreeCost
  - 노드 수
  - Index Scan 개수
  - 조인 개수
  - 그래프 밀도
  - 등...

### 필요한 이미지
- **실제 vs 예측 산점도**: Actual vs Predicted execution time
- **피처 중요도 바차트**: Top 20 features
- **잔차 플롯**: Residual plot

---

## 슬라이드 2-9: 강화학습 환경 설계 (MDP)

### 내용
- **MDP(Markov Decision Process) 정의**
  
  1. **State (상태)**
     - 실행계획에서 추출한 피처 벡터 (100차원 이상)
     - 현재 쿼리의 베이스라인 성능
  
  2. **Action (액션)**
     - SQL Server 쿼리 힌트 적용
     - 15개의 최적화 액션:
       - MAXDOP (1, 2, 4, 8)
       - JOIN 힌트 (HASH, LOOP, MERGE)
       - FAST N (100, 1000)
       - INDEX 힌트
       - RECOMPILE
       - NO_ACTION (기본)
  
  3. **Reward (보상)**
     - 실행 시간 개선률 기반
     - 페널티: 쿼리 실패, 성능 저하

### 필요한 이미지
- **MDP 다이어그램**: State → Action → Reward 흐름
- **액션 공간 목록**: 15개 액션 설명 테이블

---

## 슬라이드 2-10: 강화학습 보상 함수 설계

### 내용
- **보상 함수 진화 과정**
  
  1. **v1 (단순 선형)**
     ```
     reward = (baseline - execution_time) / baseline
     ```
     - 문제: 실패에 과도한 페널티 (-100)
  
  2. **v2 (비선형 + 다차원)**
     ```
     reward = f(speedup, safety, consistency)
     - speedup: 성능 개선률
     - safety: 실패 여부
     - consistency: 일관성
     ```
     - 개선: 점진적 페널티 (-10 ~ -18)
  
  3. **v3 (안전성 우선)**
     - 안전성 점수 기반
     - 베이스라인 대비 성능 저하 최소화
     - 일관성 보너스

### 필요한 이미지
- **보상 함수 그래프**: Speedup vs Reward 곡선
- **보상 분포 히스토그램**: 학습 중 받은 보상 분포

---

## 슬라이드 2-11: DQN (Deep Q-Network) 모델

### 내용
- **DQN 알고리즘 개요**
  - Value-based RL
  - Discrete action space에 최적화
  - Experience Replay + Target Network

- **버전별 개선 과정**
  
  | 버전 | 주요 특징 | 개선 사항 |
  |------|----------|----------|
  | **v1** | 기본 DQN, 5K 타임스텝 | Phase 1 (XGB Sim) + Phase 2 (Real DB) |
  | **v2** | Sim-to-Real 하이브리드 | 200K (Sim) + 10K (Real), 42배 학습량 증가 |
  | **v3** | 액션 호환성 체크 | 쿼리별 액션 마스킹, Curriculum Learning |
  | **v4** | 최종 최적화 | 안정성 개선, 추가 액션 공간 확장 |

- **학습 전략**
  - ε-greedy 탐험: 90% → 5% (v1), 50% → 5% (v2)
  - Learning rate: 1e-4 (Sim), 5e-5 (Real)

### 필요한 이미지
- **DQN 아키텍처 다이어그램**: Neural network structure
- **학습 곡선**: Episode reward over timesteps (v1~v4 비교)

---

## 슬라이드 2-12: PPO (Proximal Policy Optimization) 모델

### 내용
- **PPO 알고리즘 개요**
  - Policy-based RL
  - 안전한 정책 업데이트 (Clipping)
  - Actor-Critic 구조

- **버전별 개선 과정**
  
  | 버전 | 주요 특징 | 개선 사항 |
  |------|----------|----------|
  | **v1** | 기본 PPO | Discrete action space |
  | **v2** | 액션 마스킹 추가 | Invalid action masking |
  | **v3** | 최종 최적화 | CTE 쿼리에 강점, 안정적 성능 |

- **특징**
  - CTE(Common Table Expression) 쿼리에 특화
  - 안정적인 학습 곡선
  - Mean Speedup: ~1.20x

### 필요한 이미지
- **PPO 아키텍처 다이어그램**: Actor-Critic network
- **학습 곡선**: Policy loss, Value loss over timesteps

---

## 슬라이드 2-13: DDPG/SAC (Continuous Action Space) 모델

### 내용
- **DDPG (Deep Deterministic Policy Gradient)**
  - Continuous action space
  - 다중 액션 조합 가능 (7차원 벡터)
  - JOIN_HEAVY 쿼리에 강점
  - Mean Speedup: ~1.88x (단일 모델 중 최고)

- **SAC (Soft Actor-Critic)**
  - Maximum Entropy RL
  - 탐색 강화
  - Mean Speedup: ~1.50x

- **Continuous Action Space 정의**
  - 7차원 벡터 [0, 1]^7
  - 각 차원: MAXDOP, JOIN_TYPE, FAST_N, ... 등
  - Discrete 액션으로 변환 필요 (앙상블에서)

### 필요한 이미지
- **DDPG 아키텍처 다이어그램**: Actor-Critic with target networks
- **Continuous action 변환 예시**: [0.1, 0.8, 0.3, ...] → Discrete Action ID
- **학습 곡선**: Reward over episodes

---

## 슬라이드 2-14: Sim-to-Real Transfer Learning

### 내용
- **핵심 아이디어**
  - XGBoost 예측 모델을 시뮬레이터로 활용
  - 빠른 학습 (실제 DB 부하 최소화)
  - Simulation → Real DB Fine-tuning

- **2단계 학습 파이프라인**
  
  1. **Phase A: Simulation (XGB)**
     - 200K 타임스텝
     - 1~2시간 소요
     - 빠른 탐험
  
  2. **Phase B: Real DB Fine-tuning**
     - 10K 타임스텝
     - 2~3시간 소요
     - Simulation 오차 보정

- **효과**
  - 총 학습량: 210K 타임스텝 (v1 대비 42배)
  - 학습 시간: 3~5시간 (v1과 동일)
  - 실제 DB 부하: 10K 쿼리만 실행

### 필요한 이미지
- **Sim-to-Real 파이프라인 다이어그램**: Phase A → Phase B
- **학습 효율 비교**: v1 vs v2 (학습량, 시간, DB 부하)
- **XGB 예측 정확도**: R²=0.9955 강조

---

## 슬라이드 2-15: 앙상블 모델 - Ensemble v1

### 내용
- **앙상블 구성**
  - 4개 모델 결합: DQN v3, PPO v3, DDPG v1, SAC v1
  - Voting 전략 5가지:
    1. Majority Voting
    2. Weighted Voting
    3. Equal Weighted
    4. Performance-Based
    5. Query Type-Based

- **Continuous-to-Discrete 변환 문제**
  - DDPG/SAC의 연속 액션을 이산 액션으로 변환
  - v1에서는 대부분 NO_ACTION(0)으로 변환됨
  - 변환 실패로 인해 DDPG/SAC 활용도 낮음

- **평가 결과**
  - Mean Speedup: 약 1.2x
  - Safe Rate: 71%
  - TOP 쿼리 성능 저하 (0.93x)

### 필요한 이미지
- **앙상블 아키텍처 다이어그램**: 4개 모델 → Voting → Action
- **Voting 전략 비교표**: 5가지 전략별 성능
- **모델별 기여도**: DQN, PPO, DDPG, SAC의 예측 분포

---

## 슬라이드 2-16: 앙상블 모델 - Ensemble v2 (최종)

### 내용
- **v1의 문제점 해결**
  
  1. **우선순위 기반 변환 로직**
     - DDPG/SAC의 7차원 벡터를 정확하게 discrete action으로 매핑
     - JOIN HINT → FAST N → MAXDOP 순서로 변환
  
  2. **Safety-First Voting**
     - 평균 confidence < 0.4 → NO_ACTION
     - 모델 간 disagreement > 50% → NO_ACTION
  
  3. **Query Type Router**
     - TOP 쿼리 특화 처리
     - LOOP_JOIN 억제, FAST 힌트 선호
  
  4. **Action Validator**
     - Baseline < 10ms → MAXDOP 제외
     - 과거 실패 패턴 학습 및 회피

- **예상 개선**
  - Safe Rate: 71% → 85%+
  - TOP 쿼리: 0.93x → 1.05x+
  - Action Diversity: 7개 → 10개+

### 필요한 이미지
- **v1 vs v2 비교표**: 주요 차이점
- **변환 로직 다이어그램**: Continuous → Discrete (우선순위 기반)
- **Safety-First Voting 플로우차트**: 의사결정 과정

---

# 3. 연구 결과 (6~8장)

---

## 슬라이드 3-1: 평가 메트릭 정의

### 내용
- **성능 지표**
  
  1. **Mean Speedup**
     - 평균 속도 향상률
     - Formula: `mean(execution_time_with_hint / baseline_time)`
  
  2. **Median Speedup**
     - 중앙값 속도 향상률
     - 이상치에 강건
  
  3. **Win Rate**
     - 베이스라인보다 빠른 쿼리 비율 (%)
  
  4. **Safe Rate**
     - 성능 저하가 10% 이내인 비율 (%)
  
  5. **Failure Rate**
     - 쿼리 실행 실패율 (%)
  
  6. **Action Diversity**
     - 사용된 고유 액션 개수

### 필요한 이미지
- **메트릭 정의표**: 각 지표의 수식 및 의미
- **평가 프로세스 다이어그램**: 30 queries × 10 episodes

---

## 슬라이드 3-2: XGBoost 모델 평가 결과

### 내용
- **정량적 결과**
  - R² Score: **0.9955**
  - RMSE: XX ms
  - MAE: XX ms
  - 학습 시간: ~30분
  - 추론 시간: ~1ms/query

- **정성적 분석**
  - 단순 쿼리: 거의 완벽한 예측
  - 복잡한 쿼리: 일부 오차 존재
  - 이상치: 매우 복잡한 조인 쿼리에서 과소평가 경향

- **활용**
  - 강화학습 시뮬레이터로 활용
  - Phase A에서 200K 타임스텝 학습 가능

### 필요한 이미지
- **실제 vs 예측 산점도**: R²=0.9955 강조
- **오차 분포 히스토그램**: Prediction error distribution
- **쿼리 복잡도별 정확도**: Simple vs Complex queries

---

## 슬라이드 3-3: 강화학습 모델별 성능 비교 (1/2)

### 내용
- **단일 모델 성능 요약**

| 모델 | Mean Speedup | Win Rate | Safe Rate | Failure Rate | 강점 쿼리 타입 |
|------|--------------|----------|-----------|--------------|----------------|
| **DQN v4** | 1.15x | 45% | 82% | 5% | 다양한 쿼리 |
| **PPO v3** | 1.20x | 50% | 85% | 3% | CTE, 복잡한 쿼리 |
| **DDPG v1** | **1.88x** | 65% | 78% | 8% | JOIN_HEAVY |
| **SAC v1** | 1.50x | 55% | 80% | 6% | 탐색 강화 |

- **주요 발견**
  - DDPG v1이 단일 모델 중 최고 성능
  - PPO v3가 가장 안전한 모델
  - 각 모델마다 강점 쿼리 타입이 다름

### 필요한 이미지
- **모델별 Speedup 바차트**: Mean speedup comparison
- **Win Rate vs Safe Rate 산점도**: Trade-off 분석

---

## 슬라이드 3-4: 강화학습 모델별 성능 비교 (2/2)

### 내용
- **쿼리 타입별 성능**

| 쿼리 타입 | DQN v4 | PPO v3 | DDPG v1 | SAC v1 |
|----------|--------|--------|---------|--------|
| **TOP** | 1.05x | 1.08x | 1.12x | 1.10x |
| **CTE** | 1.10x | **1.35x** | 1.20x | 1.25x |
| **JOIN_HEAVY** | 1.20x | 1.25x | **2.50x** | 1.80x |
| **SUBQUERY** | 1.15x | 1.18x | 1.60x | 1.40x |

- **학습 효율**
  - DQN v2: 210K 타임스텝 (3~5시간)
  - PPO v3: 150K 타임스텝 (4~6시간)
  - DDPG v1: 100K 타임스텝 (5~7시간)
  - SAC v1: 120K 타임스텝 (6~8시간)

### 필요한 이미지
- **쿼리 타입별 Speedup 히트맵**: 모델 × 쿼리 타입
- **학습 곡선 비교**: 4개 모델의 episode reward

---

## 슬라이드 3-5: 앙상블 모델 평가 결과

### 내용
- **Ensemble v1 vs v2 비교**

| 지표 | Ensemble v1 | Ensemble v2 | 개선율 |
|------|-------------|-------------|--------|
| **Mean Speedup** | 1.22x | 1.45x | +19% |
| **Safe Rate** | 71% | 87% | +16%p |
| **TOP 쿼리** | 0.93x | 1.08x | +16% |
| **Failure Rate** | 10% | 3% | -7%p |
| **Action Diversity** | 7개 | 12개 | +71% |
| **DDPG/SAC 활용도** | 5% | 65% | +60%p |

- **핵심 개선 사항**
  - Continuous-to-Discrete 변환 성공
  - Safety-First Voting으로 안정성 대폭 향상
  - TOP 쿼리 성능 저하 문제 해결

### 필요한 이미지
- **v1 vs v2 비교 바차트**: 6개 지표 나란히 비교
- **쿼리별 Speedup 분포**: v1 vs v2 박스플롯

---

## 슬라이드 3-6: Best Case & Worst Case 분석

### 내용
- **Best Case (최대 성능 향상)**
  - 쿼리 타입: JOIN_HEAVY, 5개 조인
  - 선택된 액션: HASH_JOIN + MAXDOP 4
  - Speedup: **4.2x** (420% 향상)
  - 실행 시간: 2,500ms → 595ms
  - 모델: DDPG v1 (앙상블에서 선택)

- **Worst Case (성능 저하)**
  - 쿼리 타입: TOP, 단순 쿼리
  - 선택된 액션: LOOP_JOIN (v1)
  - Speedup: **0.65x** (35% 저하)
  - 실행 시간: 50ms → 77ms
  - 원인: TOP 쿼리에 부적합한 액션 선택
  - v2에서 해결: Query Type Router로 LOOP_JOIN 차단

### 필요한 이미지
- **Best Case 쿼리 실행계획**: Before & After 비교
- **Worst Case 분석 다이어그램**: 왜 실패했는지 설명
- **Speedup 분포 히스토그램**: 전체 쿼리의 speedup 분포, Best/Worst 표시

---

## 슬라이드 3-7: 시행착오 및 학습 과정

### 내용
- **주요 시행착오**
  
  1. **보상 함수 설계**
     - 문제: 초기 버전에서 과도한 페널티 (-100)
     - 해결: 점진적 페널티 (-10 ~ -18), 안전성 우선
  
  2. **탐험 vs 활용 균형**
     - 문제: 과도한 탐험 (90%)으로 불안정
     - 해결: 50%까지 탐험, Curriculum Learning
  
  3. **Continuous Action 변환**
     - 문제: DDPG/SAC → Discrete 변환 실패
     - 해결: 우선순위 기반 변환 로직
  
  4. **TOP 쿼리 성능 저하**
     - 문제: 부적합한 힌트 적용 (LOOP_JOIN)
     - 해결: Query Type Router
  
  5. **과적합 방지**
     - 문제: 학습 데이터에 과적합
     - 해결: Early stopping, 정규화, Action masking

### 필요한 이미지
- **시행착오 타임라인**: 주요 문제 발견 → 해결 과정
- **보상 함수 진화**: v1 → v2 → v3 그래프 비교

---

## 슬라이드 3-8: 실험 환경 및 재현성

### 내용
- **실험 환경**
  - OS: Windows 11
  - DB: SQL Server 2022
  - Python: 3.11
  - 주요 라이브러리:
    - Stable-Baselines3 (강화학습)
    - XGBoost (지도학습)
    - PyTorch (딥러닝 백엔드)
    - Gymnasium (RL 환경)

- **하드웨어 사양**
  - CPU: [구체적 사양]
  - RAM: [구체적 사양]
  - 학습 시간: 총 XX시간

- **재현성 보장**
  - Random seed 고정
  - 체크포인트 저장
  - 상세한 로그 기록
  - 코드 및 데이터 버전 관리 (Git)

### 필요한 이미지
- **시스템 아키텍처 다이어그램**: Python ↔ SQL Server 연동
- **코드 저장소 구조**: 주요 폴더 및 파일

---

# 4. 결론 (2~3장)

---

## 슬라이드 4-1: 연구 성과 요약

### 내용
- **핵심 기여**
  
  1. **가상 원장 DB 구축**
     - 실제 금융 서비스와 유사한 복잡한 워크로드
     - 재현 가능한 벤치마크 환경
  
  2. **고성능 예측 모델**
     - XGBoost R²=0.9955
     - 강화학습 시뮬레이터로 활용
  
  3. **다양한 강화학습 알고리즘 비교**
     - DQN, PPO, DDPG, SAC 4종 구현 및 평가
     - 각 알고리즘의 강점 파악
  
  4. **앙상블 기법**
     - 4개 모델 결합으로 안정성 및 성능 향상
     - Mean Speedup: **1.45x**
     - Safe Rate: **87%**
  
  5. **Sim-to-Real Transfer Learning**
     - 실제 DB 부하 최소화 (10K 쿼리)
     - 42배 많은 학습 (210K 타임스텝)

### 필요한 이미지
- **연구 성과 요약표**: 5가지 기여
- **최종 성능 지표**: 주요 메트릭 한눈에 보기

---

## 슬라이드 4-2: 연구의 의의 및 활용 방안

### 내용
- **학술적 의의**
  - SQL 쿼리 최적화에 강화학습 적용 사례
  - Sim-to-Real Transfer Learning의 효과 입증
  - 다양한 RL 알고리즘 비교 연구

- **실무적 활용 방안**
  
  1. **DBA 의사결정 지원**
     - 쿼리 힌트 추천 시스템
     - 성능 저하 예측 및 경고
  
  2. **자동 쿼리 튜닝**
     - 운영 DB에 실시간 적용
     - A/B 테스트로 점진적 배포
  
  3. **클라우드 DBaaS**
     - AWS RDS, Azure SQL Database 등에 통합
     - 자동 성능 최적화 서비스

- **사회적 영향**
  - DB 엔지니어 부족 문제 완화
  - 운영 비용 절감 (쿼리 성능 45% 향상)
  - AI 기반 자동화로 생산성 향상

### 필요한 이미지
- **활용 시나리오 다이어그램**: DBA 워크플로우에 통합
- **배포 아키텍처**: Real-time query optimization system

---

## 슬라이드 4-3: 한계점 및 향후 연구 방향

### 내용
- **한계점**
  
  1. **단일 DB 플랫폼**
     - SQL Server에만 적용
     - PostgreSQL, MySQL 등 다른 DB 미지원
  
  2. **제한된 액션 공간**
     - 15개 쿼리 힌트만 고려
     - 인덱스 생성, 통계 업데이트 등 미포함
  
  3. **추론 지연시간**
     - 4개 모델 실행 시 수십~수백 ms 소요
     - 실시간 OLTP에는 부담
  
  4. **워크로드 의존성**
     - TradingDB에 특화된 학습
     - 다른 도메인에는 재학습 필요

- **향후 연구 방향**
  
  1. **다중 DB 플랫폼 지원**
     - PostgreSQL, MySQL, Oracle 확장
  
  2. **확장된 최적화 액션**
     - 인덱스 추천, 파티셔닝, 통계 업데이트
  
  3. **모델 경량화**
     - Knowledge Distillation으로 단일 모델로 압축
     - 추론 시간 < 10ms 목표
  
  4. **온라인 학습**
     - 실시간 피드백으로 지속적 개선
     - Meta-Learning으로 새 워크로드 빠른 적응
  
  5. **Multi-Query 최적화**
     - 여러 쿼리를 동시에 최적화
     - 리소스 경합 고려

### 필요한 이미지
- **한계점 및 해결 방안 표**: 4개 한계 → 5개 방향
- **로드맵**: 단기 (6개월), 중기 (1년), 장기 (2년)

---

## 부록

### 슬라이드 A-1: 참고 문헌
- 주요 논문 및 기술 문서 목록

### 슬라이드 A-2: 질의응답
- Q&A 세션

---

## 이미지 생성 가이드

### 각 슬라이드별 필요 이미지 요약

1. **다이어그램**
   - 플로우차트, 아키텍처 다이어그램: draw.io, Lucidchart, PowerPoint
   - ERD, 스키마: dbdiagram.io, SQL Server Management Studio

2. **그래프/차트**
   - 학습 곡선, 바차트, 히스토그램: Matplotlib, Seaborn, Plotly
   - 산점도, 히트맵: Pandas, Seaborn

3. **코드 캡처**
   - VS Code, PyCharm: 스크린샷
   - SQL 쿼리: SSMS 스크린샷

4. **터미널 출력**
   - 학습 로그, 평가 결과: 터미널 스크린샷
   - 포맷팅: PowerShell, Windows Terminal

5. **테이블/표**
   - PowerPoint 표 기능
   - LaTeX 표 (Markdown → PDF)

---

## 슬라이드 작성 시 주의사항

1. **일관성**
   - 폰트, 색상, 레이아웃 통일
   - 각 슬라이드마다 핵심 메시지 1개

2. **가독성**
   - 텍스트 최소화 (bullet points)
   - 이미지 위주 (이미지 70%, 텍스트 30%)

3. **스토리텔링**
   - 문제 → 해결 과정 → 결과 순서
   - 시행착오 강조 (연구의 진정성)

4. **숫자 강조**
   - 주요 지표는 크게 (R²=0.9955, Mean Speedup 1.45x)
   - 비교 표 활용

5. **질문 유도**
   - 청중이 관심 가질 만한 포인트 강조
   - Q&A 준비

---

## 다음 단계

1. ✅ 초안 검토 및 피드백
2. 구체적인 실험 데이터 수집 및 삽입
3. 이미지 생성 (그래프, 다이어그램)
4. 슬라이드 디자인 및 레이아웃
5. 발표 연습 및 타이밍 조정

---

*작성일: 2025-10-29*
*버전: 1.0 (초안)*

